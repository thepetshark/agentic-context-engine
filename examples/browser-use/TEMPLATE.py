#!/usr/bin/env python3
"""
ACE + Browser-Use Template

Clean template for creating your own self-improving browser automation agent.
Copy this file and customize for your specific use case.

Usage:
    1. Copy this file: cp TEMPLATE.py my_use_case.py
    2. Customize the TaskEnvironment class for your task
    3. Define your samples (questions/tasks)
    4. Run and watch ACE learn!
"""

import asyncio
from pathlib import Path
from typing import List
from dotenv import load_dotenv

load_dotenv()

# ACE Framework imports
from ace import (
    LiteLLMClient,
    Generator,
    Reflector,
    Curator,
    OnlineAdapter,
    Sample,
    TaskEnvironment,
    EnvironmentResult,
    Playbook,
)

# Browser-use imports
from browser_use import Agent, Browser, ChatOpenAI

# Optional: Use v2.1 prompts for best performance
# from ace.prompts_v2_1 import PromptManager

# Optional: Import shared utilities (see shared.py for available functions)
# from shared import calculate_timeout_steps, MAX_RETRIES
# from debug import print_history_details  # For debugging


class MyTaskEnvironment(TaskEnvironment):
    """
    Environment that evaluates your browser automation task.

    This class bridges ACE with browser-use:
    1. Receives strategy from Generator
    2. Executes browser automation
    3. Returns feedback to Reflector
    """

    def __init__(self, headless: bool = True, browser_model: str = "gpt-4o-mini"):
        """
        Initialize the environment.

        Args:
            headless: Whether to run browser in headless mode
            browser_model: LLM model to use for browser-use agent
        """
        self.headless = headless
        self.browser_model = browser_model

    def evaluate(self, sample: Sample, generator_output):
        """
        Execute browser automation and return feedback.

        Args:
            sample: Contains the question/task and context
            generator_output: Strategy generated by ACE Generator

        Returns:
            EnvironmentResult with feedback and metrics
        """
        # Extract task and strategy
        task = sample.context
        strategy = generator_output.final_answer

        # TODO: Customize this for your use case
        print(f"üéØ Task: {sample.question}")
        print(f"üìù Strategy: {strategy}")

        # Run browser automation
        result = asyncio.run(self._run_browser_task(task, strategy))

        # Evaluate the result
        success = result.get("success", False)
        steps = result.get("steps", 0)
        error = result.get("error", None)

        # TODO: Customize feedback for your task
        if success:
            feedback = f"Task completed successfully in {steps} steps."
        else:
            feedback = f"Task failed after {steps} steps. Error: {error}"

        return EnvironmentResult(
            feedback=feedback,
            ground_truth=None,  # Optional: Add expected output
            metrics={
                "success": success,
                "steps": steps,
                "efficient": steps <= 10,  # Adjust threshold
            },
        )

    async def _run_browser_task(self, task: str, strategy: str):
        """
        Execute the browser automation.

        TODO: Customize this method for your specific browser task.
        """
        browser = None
        try:
            # Start browser
            browser = Browser(headless=self.headless)
            await browser.start()

            # Create browser-use agent with task + strategy
            llm = ChatOpenAI(model=self.browser_model, temperature=0.0)

            # TODO: Customize the prompt format
            full_task = f"""
            {task}

            Use this strategy:
            {strategy}
            """

            agent = Agent(
                task=full_task,
                llm=llm,
                browser=browser,
                max_actions_per_step=5,
                max_steps=15,
            )

            # Run with timeout
            history = await asyncio.wait_for(agent.run(), timeout=180.0)

            # TODO: Customize success criteria
            success = (
                history.is_successful() if hasattr(history, "is_successful") else False
            )
            steps = (
                history.number_of_steps() if hasattr(history, "number_of_steps") else 0
            )

            return {"success": success, "steps": steps, "error": None}

        except asyncio.TimeoutError:
            return {"success": False, "steps": 15, "error": "Timeout"}
        except Exception as e:
            return {"success": False, "steps": 0, "error": str(e)}
        finally:
            if browser:
                try:
                    await browser.stop()
                except:
                    pass


def main():
    """Main function - ACE online learning for your task."""

    print("\nü§ñ ACE Browser Agent")
    print("‚ú® Learning enabled - improves after each task")
    print("=" * 50)

    # TODO: Define your tasks
    tasks = [
        "Task 1: Do something on the web",
        "Task 2: Do another thing",
        "Task 3: Do a third thing",
    ]

    print(f"üìã Running {len(tasks)} tasks:")
    for i, task in enumerate(tasks, 1):
        print(f"  {i}. {task}")

    # Create ACE components
    # TODO: Choose your LLM model
    llm = LiteLLMClient(model="gpt-4o-mini", temperature=0.3)

    # Optional: Use v2.1 prompts (recommended)
    # manager = PromptManager()
    # generator = Generator(llm, prompt_template=manager.get_generator_prompt())
    # reflector = Reflector(llm, prompt_template=manager.get_reflector_prompt())
    # curator = Curator(llm, prompt_template=manager.get_curator_prompt())

    adapter = OnlineAdapter(
        playbook=Playbook(),
        generator=Generator(llm),  # Or use custom prompts above
        reflector=Reflector(llm),
        curator=Curator(llm),
        max_refinement_rounds=2,
    )

    # Create environment
    environment = MyTaskEnvironment(
        headless=False,  # Set to True for faster execution
        browser_model="gpt-4o-mini",
    )

    # Create samples
    # TODO: Customize the question format
    samples = []
    for task in tasks:
        samples.append(
            Sample(
                question=f"Complete this task: {task}",
                ground_truth=None,  # Optional: Add expected output
                context=task,
            )
        )

    print("\nüîÑ Starting ACE learning...\n")

    # Run OnlineAdapter - learns after each task!
    results = adapter.run(samples, environment)

    # Show results
    print("\n" + "=" * 50)
    print("üìä RESULTS")
    print("=" * 50)

    successful = sum(
        1 for r in results if r.environment_result.metrics.get("success", False)
    )
    total_steps = sum(r.environment_result.metrics.get("steps", 0) for r in results)
    avg_steps = total_steps / len(results) if results else 0

    print(
        f"\n‚úÖ Success rate: {successful}/{len(results)} ({100 * successful / len(results):.1f}%)"
    )
    print(f"‚ö° Average steps: {avg_steps:.1f}")

    # Show learned strategies
    if adapter.playbook.bullets():
        print(f"\nüéØ Learned Strategies:")
        for i, bullet in enumerate(adapter.playbook.bullets(), 1):
            print(f"  {i}. {bullet.content}")

    # Optional: Save playbook
    playbook_path = Path("my_task_playbook.json")
    adapter.playbook.save_to_file(str(playbook_path))
    print(f"\nüíæ Playbook saved to {playbook_path}")

    print("\n‚ú® Done! ACE learned from your tasks.")


if __name__ == "__main__":
    main()
